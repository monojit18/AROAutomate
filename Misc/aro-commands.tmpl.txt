#Prerequisites
#Installation of Tools/Softwares

- **Docker** - 
  - Windows - https://docs.docker.com/docker-for-windows/install/
  - Linux - https://runnable.com/docker/install-docker-on-linux
- **Kubectl** - 
  - Windows - https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-binary-with-curl-on-windows
  - Linux - https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-on-linux
- **Azure CLI** -
  - Windows - https://docs.microsoft.com/en-us/cli/azure/install-azure-cli-windows?tabs=azure-cli
  - Linux - https://docs.microsoft.com/en-us/cli/azure/install-azure-cli-apt#install
- **Helm** -
  - Windows and Linux - https://helm.sh/docs/intro/install/
- **Git** - 
  - Windows - https://git-scm.com/download/win
  - Linux - https://git-scm.com/book/en/v2/Getting-Started-Installing-Git
- **GitHub Desktop** (*Windows only*) -
  - Windows - https://desktop.github.com/
- **PowerShell Core** -
  - Windows and Linux - 
    - https://docs.microsoft.com/en-us/powershell/scripting/install/installing-powershell?view=powershell-7.1
- **VS Code IDE** -
  - Windows - https://code.visualstudio.com/

#Login to the Azure account
az login

#Account set (for multiple subscriptions)
az account set -s <subscription_id>

#Register Provider
az provider register -n Microsoft.RedHatOpenShift --wait
az provider register -n Microsoft.Compute --wait
az provider register -n Microsoft.Storage --wait

#Run the following command to install the az aro extension
az extension add -n aro --index https://az.aroapp.io/stable

#If you already have the extension installed, you can update by running the following command.
az extension update -n aro --index https://az.aroapp.io/stable

#Get a Red Hat pull secret (optional)
https://cloud.redhat.com/openshift/install/azure/aro-provisioned (Register or Login)

tenantId=""
subscriptionId=""
resourceGroup="aro-workshop-rg"
clusterName="aroworkshopcluster"
domain="internal.wkshpdev.com"
vnetName="aro-workshop-vnet"
vnetIPAddress="20.0.0.0/21"
masterSubnetName="master"
masterIPAddress="20.0.0.0/23"
workerSubnetName="worker"
workerIPAddress="20.0.2.0/23"
servicePrincipalName="https://$clusterName-sp"
location="eastus"
workerCount=3
clusterType="Private"
ingressType="Private"
baseFolderPath="/Users/monojitdattams/Development/Projects/Workshops/AROWorkshop/AROAutomate/Deployments"
setupFolderPath="$baseFolderPath/Setup"

az vm list-usage -l $location --query "[?contains(name.value, 'standardDSv3Family')]" -o table

#Create a virtual network containing two empty subnets
az network vnet create --resource-group $resourceGroup --name $vnetName --address-prefixes $vnetIPAddress

#Add an empty subnet for the master nodes
az network vnet subnet create --name $masterSubnetName --resource-group $resourceGroup --vnet-name $vnetName \
--address-prefixes $masterIPAddress --service-endpoints Microsoft.ContainerRegistry

#Add an empty subnet for the worker nodes
az network vnet subnet create --name  $workerSubnetName --resource-group $resourceGroup --vnet-name $vnetName \
--address-prefixes $workerIPAddress --service-endpoints Microsoft.ContainerRegistry

#Disable subnet private endpoint policies
az network vnet subnet update --name $masterSubnetName --resource-group $resourceGroup --vnet-name $vnetName \
--disable-private-link-service-network-policies true

#Create Service Principal for ARO cluster
az ad sp create-for-rbac --role Contributor -n $servicePrincipalName
#Note down the response
{
  "appId": "",
  "displayName": "https://aro-workshop-cluster-sp",
  "name": "",
  "password": "",
  "tenant": ""
}

spAppId=""
spPassword=""

#Create the ARO Cluster (Public/Private)
#$pullSecret = Get-Content "$setupFolderPath/Security/pull-secret.txt" | out-string

az aro create --resource-group $resourceGroup --vnet-resource-group $resourceGroup --location $location \
  --name $clusterName --domain $domain --vnet $vnetName --master-subnet $masterSubnetName --worker-subnet $workerSubnetName \
  --apiserver-visibility $clusterType --ingress-visibility $ingressType --worker-count $workerCount \
  --client-id $spAppId --client-secret $spPassword --pull-secret @"$setupFolderPath/Security/pull-secret.txt"

#ARO cluster details
creds=$(az aro list-credentials -n $clusterName -g $resourceGroup)
echo $creds

domain=$(az aro show -n $clusterName -g $resourceGroup --query clusterProfile.domain -o tsv)
echo $domain

location=$(az aro show -n $clusterName -g $resourceGroup --query location -o tsv)
echo $location

apiServer=$(az aro show -n $clusterName -g $resourceGroup --query apiserverProfile.url -o tsv)
echo $apiServer

webConsole=$(az aro show -n $clusterName -g $resourceGroup --query consoleProfile.url -o tsv)
echo $webConsole

oauthCallbackURL="https://oauth-openshift.apps.$domain.$location.aroapp.io/oauth2callback/AAD"
echo $oauthCallbackURL

az aro show -n $clusterName -g $resourceGroup --query '{api:apiserverProfile.ip, ingress:ingressProfiles[0].ip}'

AAD
{
  client_id: ""
  client_secret: ""

}

//kube:admin login - a temporary cluster admin
oc login $apiServer -u <user_name> -p <password>

#Configure Azure AD for RBAC
https://docs.microsoft.com/en-us/azure/openshift/configure-azure-ad-ui

#Ask each user to login with AAD credentials as described in the above link
#kube:admin user would see all users as they login from their respective console(s) for the first time

#Goto User Management -> Groups in Web console
#Create Groups using web console with these users - at least 3 (recommended) - clusteradmins, architects, developers

#RBAC
#Deploy Cluster Admins
oc apply -f "path/to/cluster_admin_rbac_file_name" (specify group name as clusteradmins)

#Deploy Cluster Managers
oc apply -f "path/to/cluster_managers_rbac_file_name" (specify group name as architects)

#Deploy Developers
oc apply -f "path/to/developers_rbac_file_name" (specify group name as developers)

#Someone from clusteradmins group can now login using AAD credentials
#Perform all subsequent cluster configurations

# Network Policy
1. NetPol folder in source repo would contain sample *Network Policy* file.This can be modified to create more specific policies
2. Define Ingress policy for each micro service (aka tyoe of PODs)
3. Define Egress policy for each micro service (aka tyoe of PODs)
4. Include any socaial IPs to be allowed
5. Exclude any IPs to Disallowed
6. Define Egress Firewall Network policy to restrict Pods accessing external links

# Decide on the communication within cluster

# Refer to Netpol folder in source

# Define Ingress and Egress of each Pod, Namespace with aprropriate access (specific to application needs)

# Deploy a sample network policy for reference
oc apply -f "path/to/netpol_file_name"

# Define Egress Firewall Network Policy
# This is to make sure what external links the Pods within the cluster can access
# Deploy a sample Egress network policy for reference
oc apply -f "path/to/egress_netpol_file_name"

**Secrets**

- Create All Secrets needed by the micro services
- Majorly Docker Secrets, Storage Secrets, DB Secrets etc.
- This can be done using oc command line Or through Web Console

#Install Nginx Ingress Controller - with Internal Load Balancer

helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx

#Install Nginx Ingress controller as Private (ILB) or Public
# Refer to Ingress folder in source repo
#For Public, comment out the following section from config file
/* service:
    loadBalancerIP: <place_holder> #private IP */

helm install nginx-ingress ingress-nginx/ingress-nginx --namespace kube-system -f "path/to/ingress_config_file_name"

#If Nginx Ingress controller is installed as Public LB
#The flow then would be - 
#Nginx Ingress Controller (Public IP) -> Kubernetes Ingress -> Backend APIs

#If Nginx Ingress controller is installed as Private LB then another External LB is needed
#to communicate with Nginx Ingress and then subsequently to the APIs in ARO cluster
#e.g. Application Gateway
#The flow then would be - 
#App G/W -> Nginx Ingress Controller (Private IP) -> Kubernetes Ingress -> Backend APIs

#uninstall nginx ingress controller
helm uninstall nginx-ingress -n kube-system

#Deploy an Ingress object
# Refer to Ingress folder in source repo
oc apply -f "path/to/ingress_file_name"

$projectName = "<place_holder>" (Same as namespace in k8s)
#Create Projects/Namespaces - Dev, QA, Staging
oc new-project $projectName
# e.g. aro-workshop-dev

#Deploy nginx server (for testing and health probe)
oc new-app nginxinc/nginx-unprivileged

oc create -n openshift -f https://raw.githubusercontent.com/openshift/origin/4ea9e6c5961eb815c200df933eee30c48a5c9166/examples/db-templates/mongodb-persistent-template.json
oc process openshift//mongodb-persistent -p MONGODB_USER=ratingsuser -p MONGODB_PASSWORD=ratingspassword -p MONGODB_DATABASE=ratingsdb -p MONGODB_ADMIN_PASSWORD=ratingspassword | oc create -f -

oc new-app https://github.com/monojit18/mslearn-aks-workshop-ratings-web.git
#Go to Deployments in Web Console
#Select ratingswebaro deployment entry
#Select Environments -> Add an entry
API = http://ratingsapiaro.aro-workshop-dev.svc.cluster.local


oc new-app https://github.com/monojit18/mslearn-aks-workshop-ratings-api.git
#Go to Deployments
#Select ratingsapiaro deployment entry
#Select Environments -> Add an entry
MONGODB_URI = mongodb://ratingsuser:ratingspassword@mongodb:27017/ratingsdb

#Test the flow using Routes
oc expose svc/ratingswebaro
#Go to Web console
#Select Routes
#Select the newly generated route and test the app flow

#Hands-on-workshop
https://aroworkshop.io/


oc new-project ostoy


Cleeanup
=========
#Delete a cluster
az aro delete -n $clusterName -g $resourceGroup -y
az network vnet delete -n $vnetName -g $resourceGroup



